---
title: "Cleaning"
author: "Francesca Giacco"
date: "2023-02-22"
output: html_document
---

```{r}
#load packages 
library(tidyverse)
library(tidyr)
library(forcats)
library(stm)
library(quanteda)
library(quanteda.textstats)
library(stringr)
```

```{r}
#load data 

#scraped articles
d20<- read_csv("df2021.csv")
head(d20)


#get one line for every article
v<- paste("l_", letters, sep="")

d1<- d20 %>% separate(links, v,  "'https?://" )%>% 
  mutate(victim=tolower(victim))


d2<-d1 %>%
  select(-c(l_a)) %>% 
  pivot_longer(
    cols = starts_with("l_"),
    names_to = "article",
    names_prefix = "wk",
    values_to = "link",
    values_drop_na = TRUE
  ) 

d3<- d2 %>%  separate(link, c("link", "title", "newspaper"),  "('|\"), " ) 

head(d3)

#save dataset
write.csv(d3, "d3.csv", row.names=FALSE)


#dataset with info about femicide - "Non una di meno"
info21<- read_csv("data2021.csv")

clean_info<- info21 %>%  
             mutate(victim=tolower(paste0(Nome," ", Cognome)))



#merge datasets (cleaned manually)

data21<- read.csv("data2021-new2.csv", sep=';' , header=TRUE) %>% 
    mutate(victim=tolower(paste0(Nome," ", Cognome)))

d4<- read.csv("d4.csv", sep=';' , header=T)
  

#merge the two datasets  by victim name 
merged2<- merge(data21, d4, by="victim", all.x=T) %>% 
        rename("date"="Data.morte") %>% 
        mutate(date= as.Date(date, format = "%d-%b-%Y"))  %>% 
        arrange(date) %>%  
        mutate(newspaper=gsub('[[:punct:] ]+', ' ',newspaper))
          

merged3<- merged2 %>% 
        mutate(link= ifelse(is.na(link), Link.fonte.media, link))

#save dataset - this dataset contains all the links, not the article texts 
#

write.csv(merged3, "merged.csv", row.names=FALSE)


```
I have added the texts to all the links (manually). In a later stage or for reproduction purpose, better to use the package "Newsplease" on Python. 
The new file is called "merged-merged.csv"

```{r}
datac<- read.csv("merged-merged.csv")

colnames(datac)

datac<- datac %>% 
  mutate(text=str_replace_all(text, "[\r\n]" , " ")) %>% 
  subset((!is.na(text))) %>% 
  mutate(tit=str_replace_all(title, "[\r\n]" , " ")) 
  
datac <- with(datac, datac[!(text == "" | is.na(text)), ])
datac <- with(datac, datac[!(tit == "" | is.na(tit)), ]) 

write.csv(datac, "datac.csv", row.names=FALSE)



```


After having added the victim blaming scores, I need to clean the column and keep only the score. 

```{r}
dws<-read_csv("datawscores2.csv")

colnames(dws)

dws<- dws %>% 
 mutate(tit_vb= as.numeric(str_extract(tit_vb, "[0-9].[0-9]+"))) %>% 
   mutate(text_vb= as.numeric(str_extract(text_vb, "[0-9].[0-9]+")))

#check that it is numeric
class(dws$ass_blame)

#save dataset with clean scores
write.csv(dws, "dws_clean.csv", row.names=FALSE)
```


# prepare data to use the model at sentence level. 
```{r}

n<-c(1:100)

v<- paste("n_", n, sep="")

sent_datac<- datac %>% separate(text, v,  "\\." )
 

sent_datac<-sent_datac %>%
  pivot_longer(
    cols = starts_with("n_"),
    names_to = "code",
    names_prefix = "wk",
    values_to = "sentence",
    values_drop_na = TRUE
  ) 

sent_datac <- with(sent_datac, sent_datac[!(sentence == "" | is.na(sentence)), ])

sent_datac<- sent_datac %>% 
             mutate(n_char= nchar(sentence)) %>% 
             subset( n_char>34)
            

nchar(sent_datac$sentence[7])
                   
#save dataset
write.csv(sent_datac, "datac.csv", row.names=FALSE)




```

Now that I have a score for every sentence, mean over each article. 

```{r}

```





