{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francescagiacco/feminicides-italy/blob/main/scripts/05_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FINE TUNING THE PRE-TRAINED MODEL**\n",
        "\n",
        "In this script I fine tune this [model](https://huggingface.co/responsibility-framing/predict-perception-bert-blame-victim) on my data \n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ET_3HCLfWP3Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpCG7NggmBat",
        "outputId": "2e18d414-37e6-4ce9-c845-87ce9b0cdbc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.28.0 in /usr/local/lib/python3.10/dist-packages (4.28.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.28.0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.28.0) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers_interpret in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: captum>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers_interpret) (0.6.0)\n",
            "Requirement already satisfied: ipython<8.0.0,>=7.31.1 in /usr/local/lib/python3.10/dist-packages (from transformers_interpret) (7.34.0)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from transformers_interpret) (4.28.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum>=0.3.1->transformers_interpret) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum>=0.3.1->transformers_interpret) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum>=0.3.1->transformers_interpret) (2.0.0+cu118)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (67.7.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (0.18.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython<8.0.0,>=7.31.1->transformers_interpret) (4.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->transformers_interpret) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->transformers_interpret) (0.14.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->transformers_interpret) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->transformers_interpret) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->transformers_interpret) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->transformers_interpret) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->transformers_interpret) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->transformers_interpret) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers>=3.0.0->transformers_interpret) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers>=3.0.0->transformers_interpret) (4.5.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython<8.0.0,>=7.31.1->transformers_interpret) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython<8.0.0,>=7.31.1->transformers_interpret) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0.0,>=7.31.1->transformers_interpret) (0.2.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum>=0.3.1->transformers_interpret) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->captum>=0.3.1->transformers_interpret) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->captum>=0.3.1->transformers_interpret) (16.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum>=0.3.1->transformers_interpret) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=3.0.0->transformers_interpret) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=3.0.0->transformers_interpret) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=3.0.0->transformers_interpret) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=3.0.0->transformers_interpret) (3.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum>=0.3.1->transformers_interpret) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum>=0.3.1->transformers_interpret) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum>=0.3.1->transformers_interpret) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.28.0\n",
        "#!pip install news-please \n",
        "!pip install transformers_interpret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2D_x7iUFUZi3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GDupqGynJN-"
      },
      "outputs": [],
      "source": [
        "#load packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "from transformers.pipelines.text_classification import ClassificationFunction\n",
        "from transformers_interpret import SequenceClassificationExplainer\n",
        "import torch\n",
        "from torch import tensor\n",
        "from torch.nn import Sigmoid, Softmax\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import EarlyStoppingCallback\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZeX392Hp8s6",
        "outputId": "498cd166-0d34-41a2-dcb1-57fe096fd381"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyMWwxaonaBt",
        "outputId": "6549daef-9c22-4728-fa9e-0f39c88b5ee6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-a9a5458c4404>:1: DtypeWarning: Columns (14,27,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  datac=pd.read_csv(\"/content/drive/MyDrive/thesis/labeltest.csv\")\n"
          ]
        }
      ],
      "source": [
        "datac=pd.read_csv(\"/content/drive/MyDrive/thesis/labeltest.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data preprocessing**"
      ],
      "metadata": {
        "id": "toQlu3LRXbBq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJGF2BPcn5pK",
        "outputId": "c555a6b6-e04d-4e96-edd5-7d5491c8c7d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2682"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        " \n",
        "data_ft= datac.dropna(subset=['sentence', 'vb_manual']).copy()\n",
        "data_ft[\"vb_bin\"]= data_ft['vb_manual'].apply(lambda x: 0 if x <4  else 1 )\n",
        "data_ft[\"vb_class\"]=data_ft['vb_manual'].apply(lambda x: 0 if x <5  else 1 if x<10 else 2 )\n",
        "\n",
        "min_col = data_ft[\"vb_manual\"].min()\n",
        "max_col = data_ft[\"vb_manual\"].max()\n",
        "data_ft.loc[:, \"vb_manual_norm\"] = (data_ft[\"vb_manual\"] - min_col) / (max_col - min_col)\n",
        "\n",
        "max_col = data_ft[\"vb_manual_norm\"].max()\n",
        "print(max_col)\n",
        "len(data_ft)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "37u642b1QCoF",
        "outputId": "bd08f47b-746f-4551-e0d6-1faef05b2b38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGYCAYAAABcVthxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdNElEQVR4nO3df5DU9X3H8dcBcuKPO4LIHTde1CSjSPwZbPESpbEyHIo2NnZaovFHQrQ6R2aUqMjUoiadYkgaE1sNk7aGdKqtZkathQl6wQipnqJ0ToVEJhotOHiHP8KtEAWE6x8ZtrkGjSB498HHY+Y7w+73s7vvbyaXe2b3u9+r6e3t7Q0AQEEG9fcAAAA7S8AAAMURMABAcQQMAFAcAQMAFEfAAADFETAAQHEEDABQnCH9PcCesm3btqxduzYHHnhgampq+nscAOBd6O3tzeuvv56mpqYMGvT277PstQGzdu3aNDc39/cYAMAuWLNmTQ455JC33b/XBsyBBx6Y5Df/AdTV1fXzNADAu1GpVNLc3Fz9Pf52dipg5syZk7vvvjvPPPNMhg0blk9+8pP5+te/niOPPLK65tOf/nSWLFnS53F/+Zd/mXnz5lVvr169Opdddll+8pOf5IADDsiFF16YOXPmZMiQ/xvnoYceyowZM7Jy5co0Nzfn2muvzUUXXfSuZ93+sVFdXZ2AAYDC/L7TP3bqJN4lS5akra0tjz76aNrb27Nly5ZMmjQpGzdu7LPu4osvzksvvVTd5s6dW923devWTJkyJZs3b84jjzySH/zgB5k/f35mz55dXfP8889nypQpOfXUU9PZ2ZnLL788X/rSl3L//ffvzLgAwF6q5r38NeqXX345o0aNypIlSzJhwoQkv3kH5vjjj8+3v/3tHT7mRz/6Uc4888ysXbs2DQ0NSZJ58+Zl5syZefnllzN06NDMnDkzCxcuzIoVK6qPmzp1atavX59Fixa9q9kqlUrq6+vT09PjHRgAKMS7/f39nr5G3dPTkyQZMWJEn/tvv/32jBw5MkcffXRmzZqVX//619V9HR0dOeaYY6rxkiStra2pVCpZuXJldc3EiRP7PGdra2s6OjredpZNmzalUqn02QCAvdMun8S7bdu2XH755fnUpz6Vo48+unr/ueeem0MPPTRNTU156qmnMnPmzKxatSp33313kqSrq6tPvCSp3u7q6nrHNZVKJW+88UaGDRv2O/PMmTMnN9xww64eDgBQkF0OmLa2tqxYsSL/9V//1ef+Sy65pPrvY445JqNHj85pp52W5557Lh/96Ed3fdLfY9asWZkxY0b19vazmAGAvc8ufYQ0ffr0LFiwID/5yU/e8TvaSTJ+/PgkybPPPpskaWxsTHd3d5812283Nja+45q6urodvvuSJLW1tdVvHPnmEQDs3XYqYHp7ezN9+vTcc889efDBB3P44Yf/3sd0dnYmSUaPHp0kaWlpydNPP51169ZV17S3t6euri5jx46trlm8eHGf52lvb09LS8vOjAsA7KV2KmDa2tryr//6r7njjjty4IEHpqurK11dXXnjjTeSJM8991y+9rWvZfny5XnhhRdy33335YILLsiECRNy7LHHJkkmTZqUsWPH5vzzz8+TTz6Z+++/P9dee23a2tpSW1ubJLn00kvzy1/+MldffXWeeeaZ3HrrrbnrrrtyxRVX7ObDBwBKtFNfo367i8p8//vfz0UXXZQ1a9bk85//fFasWJGNGzemubk5f/qnf5prr722z0c6//M//5PLLrssDz30UPbff/9ceOGFufHGG3/nQnZXXHFFfvazn+WQQw7JX//1X+/Uhex8jRoAyvNuf3+/p+vADGQCBgDK875cBwYAoD8IGACgOAIGACiOgAEAirPLV+Jl4DrsmoX9PQLvoxdunNLfIwC877wDAwAUR8AAAMURMABAcQQMAFAcAQMAFEfAAADFETAAQHEEDABQHAEDABRHwAAAxREwAEBxBAwAUBwBAwAUR8AAAMURMABAcQQMAFAcAQMAFEfAAADFETAAQHEEDABQHAEDABRHwAAAxREwAEBxBAwAUBwBAwAUR8AAAMURMABAcQQMAFAcAQMAFEfAAADFETAAQHEEDABQHAEDABRHwAAAxREwAEBxBAwAUBwBAwAUR8AAAMURMABAcQQMAFAcAQMAFEfAAADFETAAQHEEDABQHAEDABRHwAAAxREwAEBxBAwAUBwBAwAUR8AAAMURMABAcQQMAFAcAQMAFEfAAADFETAAQHEEDABQHAEDABRnpwJmzpw5+YM/+IMceOCBGTVqVM4+++ysWrWqz5o333wzbW1tOeigg3LAAQfknHPOSXd3d581q1evzpQpU7Lffvtl1KhRueqqq/LWW2/1WfPQQw/lE5/4RGpra/Oxj30s8+fP37UjBAD2OjsVMEuWLElbW1seffTRtLe3Z8uWLZk0aVI2btxYXXPFFVfkP//zP/PDH/4wS5Ysydq1a/PZz362un/r1q2ZMmVKNm/enEceeSQ/+MEPMn/+/MyePbu65vnnn8+UKVNy6qmnprOzM5dffnm+9KUv5f77798NhwwAlK6mt7e3d1cf/PLLL2fUqFFZsmRJJkyYkJ6enhx88MG544478md/9mdJkmeeeSZHHXVUOjo6ctJJJ+VHP/pRzjzzzKxduzYNDQ1Jknnz5mXmzJl5+eWXM3To0MycOTMLFy7MihUrqq81derUrF+/PosWLXpXs1UqldTX16enpyd1dXW7eohFOuyahf09Au+jF26c0t8jAOw27/b393s6B6anpydJMmLEiCTJ8uXLs2XLlkycOLG6ZsyYMfnwhz+cjo6OJElHR0eOOeaYarwkSWtrayqVSlauXFld89vPsX3N9ufYkU2bNqVSqfTZAIC90y4HzLZt23L55ZfnU5/6VI4++ugkSVdXV4YOHZrhw4f3WdvQ0JCurq7qmt+Ol+37t+97pzWVSiVvvPHGDueZM2dO6uvrq1tzc/OuHhoAMMDtcsC0tbVlxYoV+fd///fdOc8umzVrVnp6eqrbmjVr+nskAGAPGbIrD5o+fXoWLFiQpUuX5pBDDqne39jYmM2bN2f9+vV93oXp7u5OY2Njdc2yZcv6PN/2byn99pr//82l7u7u1NXVZdiwYTucqba2NrW1tbtyOABAYXbqHZje3t5Mnz4999xzTx588MEcfvjhffaPGzcu++yzTxYvXly9b9WqVVm9enVaWlqSJC0tLXn66aezbt266pr29vbU1dVl7Nix1TW//Rzb12x/DgDgg22n3oFpa2vLHXfckf/4j//IgQceWD1npb6+PsOGDUt9fX2mTZuWGTNmZMSIEamrq8uXv/zltLS05KSTTkqSTJo0KWPHjs3555+fuXPnpqurK9dee23a2tqq76Bceuml+Yd/+IdcffXV+eIXv5gHH3wwd911VxYu9O0aAGAn34H57ne/m56ennz605/O6NGjq9udd95ZXXPTTTflzDPPzDnnnJMJEyaksbExd999d3X/4MGDs2DBggwePDgtLS35/Oc/nwsuuCBf/epXq2sOP/zwLFy4MO3t7TnuuOPyd3/3d/mnf/qntLa27oZDBgBK956uAzOQuQ4MHxSuAwPsTd6X68AAAPQHAQMAFEfAAADFETAAQHEEDABQHAEDABRHwAAAxREwAEBxBAwAUBwBAwAUR8AAAMURMABAcQQMAFAcAQMAFEfAAADFETAAQHEEDABQHAEDABRHwAAAxREwAEBxBAwAUBwBAwAUR8AAAMURMABAcQQMAFAcAQMAFEfAAADFETAAQHEEDABQHAEDABRHwAAAxREwAEBxBAwAUBwBAwAUR8AAAMURMABAcQQMAFAcAQMAFEfAAADFETAAQHEEDABQHAEDABRHwAAAxREwAEBxBAwAUBwBAwAUR8AAAMURMABAcQQMAFAcAQMAFEfAAADFETAAQHEEDABQHAEDABRHwAAAxREwAEBxBAwAUBwBAwAUR8AAAMURMABAcQQMAFCcnQ6YpUuX5qyzzkpTU1Nqampy77339tl/0UUXpaamps82efLkPmtee+21nHfeeamrq8vw4cMzbdq0bNiwoc+ap556Kqecckr23XffNDc3Z+7cuTt/dADAXmmnA2bjxo057rjjcsstt7ztmsmTJ+ell16qbv/2b//WZ/95552XlStXpr29PQsWLMjSpUtzySWXVPdXKpVMmjQphx56aJYvX55vfOMbuf766/O9731vZ8cFAPZCQ3b2AaeffnpOP/30d1xTW1ubxsbGHe77+c9/nkWLFuXxxx/PiSeemCT5+7//+5xxxhn55je/maamptx+++3ZvHlzbrvttgwdOjQf//jH09nZmW9961t9QgcA+GDaI+fAPPTQQxk1alSOPPLIXHbZZXn11Ver+zo6OjJ8+PBqvCTJxIkTM2jQoDz22GPVNRMmTMjQoUOra1pbW7Nq1ar86le/2uFrbtq0KZVKpc8GAOyddnvATJ48Of/yL/+SxYsX5+tf/3qWLFmS008/PVu3bk2SdHV1ZdSoUX0eM2TIkIwYMSJdXV3VNQ0NDX3WbL+9fc3/N2fOnNTX11e35ubm3X1oAMAAsdMfIf0+U6dOrf77mGOOybHHHpuPfvSjeeihh3Laaaft7permjVrVmbMmFG9XalURAwA7KX2+NeoP/KRj2TkyJF59tlnkySNjY1Zt25dnzVvvfVWXnvttep5M42Njenu7u6zZvvttzu3pra2NnV1dX02AGDvtMcD5sUXX8yrr76a0aNHJ0laWlqyfv36LF++vLrmwQcfzLZt2zJ+/PjqmqVLl2bLli3VNe3t7TnyyCPzoQ99aE+PDAAMcDsdMBs2bEhnZ2c6OzuTJM8//3w6OzuzevXqbNiwIVdddVUeffTRvPDCC1m8eHE+85nP5GMf+1haW1uTJEcddVQmT56ciy++OMuWLcvDDz+c6dOnZ+rUqWlqakqSnHvuuRk6dGimTZuWlStX5s4778x3vvOdPh8RAQAfXDsdME888UROOOGEnHDCCUmSGTNm5IQTTsjs2bMzePDgPPXUU/mTP/mTHHHEEZk2bVrGjRuXn/70p6mtra0+x+23354xY8bktNNOyxlnnJGTTz65zzVe6uvr88ADD+T555/PuHHj8pWvfCWzZ8/2FWoAIElS09vb29vfQ+wJlUol9fX16enp+cCdD3PYNQv7ewTeRy/cOKW/RwDYbd7t729/CwkAKI6AAQCKI2AAgOIIGACgOAIGACiOgAEAiiNgAIDiCBgAoDgCBgAojoABAIojYACA4ggYAKA4AgYAKI6AAQCKI2AAgOIIGACgOAIGACiOgAEAiiNgAIDiCBgAoDgCBgAojoABAIojYACA4ggYAKA4AgYAKI6AAQCKI2AAgOIIGACgOAIGACiOgAEAiiNgAIDiCBgAoDgCBgAojoABAIojYACA4ggYAKA4AgYAKI6AAQCKI2AAgOIIGACgOAIGACiOgAEAiiNgAIDiCBgAoDgCBgAojoABAIojYACA4ggYAKA4AgYAKI6AAQCKI2AAgOIIGACgOAIGACiOgAEAiiNgAIDiCBgAoDgCBgAojoABAIojYACA4ggYAKA4AgYAKI6AAQCKs9MBs3Tp0px11llpampKTU1N7r333j77e3t7M3v27IwePTrDhg3LxIkT84tf/KLPmtdeey3nnXde6urqMnz48EybNi0bNmzos+app57KKaeckn333TfNzc2ZO3fuzh8dALBX2umA2bhxY4477rjccsstO9w/d+7c3HzzzZk3b14ee+yx7L///mltbc2bb75ZXXPeeedl5cqVaW9vz4IFC7J06dJccskl1f2VSiWTJk3KoYcemuXLl+cb3/hGrr/++nzve9/bhUMEAPY2Nb29vb27/OCamtxzzz05++yzk/zm3ZempqZ85StfyZVXXpkk6enpSUNDQ+bPn5+pU6fm5z//ecaOHZvHH388J554YpJk0aJFOeOMM/Liiy+mqakp3/3ud/NXf/VX6erqytChQ5Mk11xzTe69994888wz72q2SqWS+vr69PT0pK6ublcPsUiHXbOwv0fgffTCjVP6ewSA3ebd/v7erefAPP/88+nq6srEiROr99XX12f8+PHp6OhIknR0dGT48OHVeEmSiRMnZtCgQXnssceqayZMmFCNlyRpbW3NqlWr8qtf/WqHr71p06ZUKpU+GwCwd9qtAdPV1ZUkaWho6HN/Q0NDdV9XV1dGjRrVZ/+QIUMyYsSIPmt29By//Rr/35w5c1JfX1/dmpub3/sBAQAD0l7zLaRZs2alp6enuq1Zs6a/RwIA9pDdGjCNjY1Jku7u7j73d3d3V/c1NjZm3bp1ffa/9dZbee211/qs2dFz/PZr/H+1tbWpq6vrswEAe6fdGjCHH354Ghsbs3jx4up9lUoljz32WFpaWpIkLS0tWb9+fZYvX15d8+CDD2bbtm0ZP358dc3SpUuzZcuW6pr29vYceeSR+dCHPrQ7RwYACrTTAbNhw4Z0dnams7MzyW9O3O3s7Mzq1atTU1OTyy+/PH/zN3+T++67L08//XQuuOCCNDU1Vb+pdNRRR2Xy5Mm5+OKLs2zZsjz88MOZPn16pk6dmqampiTJueeem6FDh2batGlZuXJl7rzzznznO9/JjBkzdtuBAwDlGrKzD3jiiSdy6qmnVm9vj4oLL7ww8+fPz9VXX52NGzfmkksuyfr163PyySdn0aJF2XfffauPuf322zN9+vScdtppGTRoUM4555zcfPPN1f319fV54IEH0tbWlnHjxmXkyJGZPXt2n2vFAAAfXO/pOjADmevA8EHhOjDA3qRfrgMDAPB+EDAAQHEEDABQHAEDABRHwAAAxREwAEBxBAwAUBwBAwAUR8AAAMURMABAcQQMAFAcAQMAFEfAAADFETAAQHEEDABQHAEDABRHwAAAxREwAEBxBAwAUBwBAwAUR8AAAMURMABAcQQMAFAcAQMAFEfAAADFETAAQHEEDABQHAEDABRHwAAAxREwAEBxBAwAUBwBAwAUR8AAAMURMABAcQQMAFAcAQMAFEfAAADFETAAQHEEDABQHAEDABRHwAAAxREwAEBxBAwAUBwBAwAUR8AAAMURMABAcQQMAFAcAQMAFEfAAADFETAAQHEEDABQHAEDABRHwAAAxREwAEBxBAwAUBwBAwAUR8AAAMURMABAcQQMAFAcAQMAFEfAAADF2e0Bc/3116empqbPNmbMmOr+N998M21tbTnooINywAEH5Jxzzkl3d3ef51i9enWmTJmS/fbbL6NGjcpVV12Vt956a3ePCgAUasieeNKPf/zj+fGPf/x/LzLk/17miiuuyMKFC/PDH/4w9fX1mT59ej772c/m4YcfTpJs3bo1U6ZMSWNjYx555JG89NJLueCCC7LPPvvkb//2b/fEuABAYfZIwAwZMiSNjY2/c39PT0/++Z//OXfccUf++I//OEny/e9/P0cddVQeffTRnHTSSXnggQfys5/9LD/+8Y/T0NCQ448/Pl/72tcyc+bMXH/99Rk6dOieGBkAKMgeOQfmF7/4RZqamvKRj3wk5513XlavXp0kWb58ebZs2ZKJEydW144ZMyYf/vCH09HRkSTp6OjIMccck4aGhuqa1tbWVCqVrFy5ck+MCwAUZre/AzN+/PjMnz8/Rx55ZF566aXccMMNOeWUU7JixYp0dXVl6NChGT58eJ/HNDQ0pKurK0nS1dXVJ16279++7+1s2rQpmzZtqt6uVCq76YgAgIFmtwfM6aefXv33sccem/Hjx+fQQw/NXXfdlWHDhu3ul6uaM2dObrjhhj32/ADAwLHHv0Y9fPjwHHHEEXn22WfT2NiYzZs3Z/369X3WdHd3V8+ZaWxs/J1vJW2/vaPzarabNWtWenp6qtuaNWt274EAAAPGHg+YDRs25Lnnnsvo0aMzbty47LPPPlm8eHF1/6pVq7J69eq0tLQkSVpaWvL0009n3bp11TXt7e2pq6vL2LFj3/Z1amtrU1dX12cDAPZOu/0jpCuvvDJnnXVWDj300KxduzbXXXddBg8enM997nOpr6/PtGnTMmPGjIwYMSJ1dXX58pe/nJaWlpx00klJkkmTJmXs2LE5//zzM3fu3HR1deXaa69NW1tbamtrd/e4AECBdnvAvPjii/nc5z6XV199NQcffHBOPvnkPProozn44IOTJDfddFMGDRqUc845J5s2bUpra2tuvfXW6uMHDx6cBQsW5LLLLktLS0v233//XHjhhfnqV7+6u0cFAApV09vb29vfQ+wJlUol9fX16enp+cB9nHTYNQv7ewTeRy/cOKW/RwDYbd7t729/CwkAKI6AAQCKI2AAgOIIGACgOAIGACiOgAEAiiNgAIDiCBgAoDgCBgAojoABAIojYACA4ggYAKA4AgYAKI6AAQCKI2AAgOIIGACgOAIGACiOgAEAiiNgAIDiCBgAoDgCBgAojoABAIojYACA4ggYAKA4AgYAKI6AAQCKI2AAgOIIGACgOAIGACjOkP4eAIB377BrFvb3CLyPXrhxSn+PMGB5BwYAKI6AAQCKI2AAgOIIGACgOAIGACiOgAEAiiNgAIDiCBgAoDgCBgAojoABAIojYACA4ggYAKA4AgYAKI6AAQCKI2AAgOIIGACgOAIGACiOgAEAiiNgAIDiCBgAoDgCBgAojoABAIojYACA4ggYAKA4AgYAKI6AAQCKI2AAgOIIGACgOAIGACiOgAEAiiNgAIDiCBgAoDgDOmBuueWWHHbYYdl3330zfvz4LFu2rL9HAgAGgAEbMHfeeWdmzJiR6667Lv/93/+d4447Lq2trVm3bl1/jwYA9LMBGzDf+ta3cvHFF+cLX/hCxo4dm3nz5mW//fbLbbfd1t+jAQD9bEh/D7AjmzdvzvLlyzNr1qzqfYMGDcrEiRPT0dGxw8ds2rQpmzZtqt7u6elJklQqlT077AC0bdOv+3sE3kcfxP+Of5D5+f5g+SD+fG8/5t7e3ndcNyAD5pVXXsnWrVvT0NDQ5/6GhoY888wzO3zMnDlzcsMNN/zO/c3NzXtkRhgo6r/d3xMAe8oH+ef79ddfT319/dvuH5ABsytmzZqVGTNmVG9v27Ytr732Wg466KDU1NT042S8HyqVSpqbm7NmzZrU1dX19zjAbuTn+4Olt7c3r7/+epqamt5x3YAMmJEjR2bw4MHp7u7uc393d3caGxt3+Jja2trU1tb2uW/48OF7akQGqLq6Ov8DB3spP98fHO/0zst2A/Ik3qFDh2bcuHFZvHhx9b5t27Zl8eLFaWlp6cfJAICBYEC+A5MkM2bMyIUXXpgTTzwxf/iHf5hvf/vb2bhxY77whS/092gAQD8bsAHzF3/xF3n55Zcze/bsdHV15fjjj8+iRYt+58ReSH7zEeJ11133Ox8jAuXz882O1PT+vu8pAQAMMAPyHBgAgHciYACA4ggYAKA4AgYAKI6AAQCKM2C/Rg3v5JVXXsltt92Wjo6OdHV1JUkaGxvzyU9+MhdddFEOPvjgfp4QgD3JOzAU5/HHH88RRxyRm2++OfX19ZkwYUImTJiQ+vr63HzzzRkzZkyeeOKJ/h4T2APWrFmTL37xi/09BgOA68BQnJNOOinHHXdc5s2b9zt/qLO3tzeXXnppnnrqqXR0dPTThMCe8uSTT+YTn/hEtm7d2t+j0M98hERxnnzyycyfP3+Hf2W8pqYmV1xxRU444YR+mAx4r+6777533P/LX/7yfZqEgU7AUJzGxsYsW7YsY8aM2eH+ZcuW+ZMTUKizzz47NTU1eacPB3b0f1744BEwFOfKK6/MJZdckuXLl+e0006rxkp3d3cWL16cf/zHf8w3v/nNfp4S2BWjR4/Orbfems985jM73N/Z2Zlx48a9z1MxEAkYitPW1paRI0fmpptuyq233lr9LHzw4MEZN25c5s+fnz//8z/v5ymBXTFu3LgsX778bQPm9707wweHk3gp2pYtW/LKK68kSUaOHJl99tmnnycC3ouf/vSn2bhxYyZPnrzD/Rs3bswTTzyRP/qjP3qfJ2OgETAAQHFcBwYAKI6AAQCKI2AAgOIIGACgOAIGACiOgAEAiiNgAIDiCBgAoDj/C80KxQCRERWpAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "counts = data_ft[\"vb_bin\"].value_counts()\n",
        "counts.plot(kind=\"bar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare for training**"
      ],
      "metadata": {
        "id": "z15AMoV7XgL2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxZ6CQUYp120"
      },
      "outputs": [],
      "source": [
        "X=list(data_ft['sentence'])\n",
        "y=list(data_ft['vb_bin'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7N4P5l5Xq7E_"
      },
      "outputs": [],
      "source": [
        "#train, validation, test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "#training and remaining dataset\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.8, random_state=0)\n",
        "\n",
        "#validation and test sets are 5% of the data each\n",
        "test_size = 0.5\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=0.5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxyEmBlP0EoY",
        "outputId": "9c41f910-2f50-40ee-bcb4-1e7842f19ae0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at responsibility-framing/predict-perception-bert-blame-assassin and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([1, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([1]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"responsibility-framing/predict-perception-bert-blame-assassin\",\n",
        "                                                           num_labels=2, ignore_mismatched_sizes=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"responsibility-framing/predict-perception-bert-blame-assassin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbIwkaV31Ltf"
      },
      "outputs": [],
      "source": [
        "encoded_train = tokenizer(X_train, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "encoded_test = tokenizer(X_test, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "encoded_test_fin = tokenizer(X_val, padding=True, truncation=True, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo2hRJXayd-G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class ThesisDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = ThesisDataset(encoded_train, y_train)\n",
        "val_dataset = ThesisDataset(encoded_test, y_test)\n",
        "test_dataset=ThesisDataset(encoded_test_fin, y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaRYH0Ic8kaq"
      },
      "outputs": [],
      "source": [
        "#change loss type\n",
        "model.config.problem_type = 'single_label_classification'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aT21n6uSoUZ",
        "outputId": "28ed025e-16ba-4d30-9324-3808c1198c0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/accelerate\n",
            "  Cloning https://github.com/huggingface/accelerate to /tmp/pip-req-build-_06cxvto\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate /tmp/pip-req-build-_06cxvto\n",
            "  Resolved https://github.com/huggingface/accelerate to commit dcde1e93d09abea02a8e7f4a07a2c5734b87b60e\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.0.dev0) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.0.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.0.dev0) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.20.0.dev0) (2.0.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.0.dev0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.0.dev0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.0.dev0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.0.dev0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate==0.20.0.dev0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate==0.20.0.dev0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate==0.20.0.dev0) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate==0.20.0.dev0) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate==0.20.0.dev0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/huggingface/accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94cYaSQlsoUU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    f1 = f1_score(labels, preds, average='macro')\n",
        "    return {'f1': f1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "79eUPYUusbmX",
        "outputId": "ba3ba43d-eda9-49bb-b545-eca05a93a8bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "<ipython-input-13-6a7b81468718>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [270/270 02:27, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.307000</td>\n",
              "      <td>0.386180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.124600</td>\n",
              "      <td>0.282686</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=270, training_loss=0.2852114227082994, metrics={'train_runtime': 148.806, 'train_samples_per_second': 28.829, 'train_steps_per_second': 1.814, 'total_flos': 438711990373800.0, 'train_loss': 0.2852114227082994, 'epoch': 2.0})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=2,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='epoch'\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset     \n",
        "        #evaluation dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "1bRwbJagWEP7",
        "outputId": "c14548ee-9302-4a6a-bd84-9c14e42fd55c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-6a7b81468718>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.23109689354896545,\n",
              " 'eval_runtime': 1.7164,\n",
              " 'eval_samples_per_second': 156.144,\n",
              " 'eval_steps_per_second': 2.913,\n",
              " 'epoch': 2.0}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "s8i1HWJ-szza",
        "outputId": "4473aec8-b7ec-42fa-bc89-5ae34417fe95"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-6a7b81468718>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "output=trainer.predict(val_dataset)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsExuSeMs4TI",
        "outputId": "2ee8d17d-0fcf-43c1-e122-94e13ab06e57"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[238,   0],\n",
              "       [  0,  31]])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm=confusion_matrix(y_test,output)\n",
        "cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywiwBf04wUE2"
      },
      "outputs": [],
      "source": [
        "trainer.save_model('finetuned_model')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}